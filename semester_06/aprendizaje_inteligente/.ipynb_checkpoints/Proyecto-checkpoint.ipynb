{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "289fc109-217f-43dd-a368-ea01cb18adef",
   "metadata": {},
   "source": [
    "# Universidad Autonoma de Aguascalientes\n",
    "### Centro de Ciencias Basicas\n",
    "### Departamento de Ciencias de la Computacion\n",
    "### Carrera: Ingenieria en Computacion Inteligente\n",
    "### Materia: Machine Learning \n",
    "### Profesor: Dr. Francisco Javier Luna Rosas\n",
    "### Proyecto Final\n",
    "### Integrantes: \n",
    "- #### Diego Alberto Aranda Gonzalez\n",
    "- #### Dante Alejandro Alegría Romero\n",
    "- #### Andrea Margarita Balandrán Félix\n",
    "- #### Diego Emilio Moreno Sánchez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39228fc7-a003-43a8-9e2b-5e9a6cf4833a",
   "metadata": {},
   "source": [
    "Este proyecto junta técnicas de scrapping para extraer datos de la web, en conjutno con técnicas de aprendizaje supervisado y no supervisado, en este caso SVM y bosques aleatorios, estos modelos serán utilizados para clasificar noticias en base a su título en 3 diferentes clasificaciones: Política, Deportes y Negocios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc79a7b-f626-4c84-b6fc-ca9753ea0d22",
   "metadata": {},
   "source": [
    "### Recolección de datos con webmining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b436ad4-3eb8-4ba1-b6e6-f67a91aa924b",
   "metadata": {},
   "source": [
    "Importamos las librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f11262-5d07-4211-bfc3-788174a8bc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ecbd0d-6224-4e1c-b5b8-f23f021655c6",
   "metadata": {},
   "source": [
    "Definimos las urls de donde vamos a extraer la información"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d95a0ad6-e316-4018-a4de-7592930b4d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_deportes = [\n",
    "    'https://www.skysports.com/football/olympiakos-vs-a-villa/report/504410',\n",
    "    'https://www.skysports.com/nfl/news/12118/13124057/2024-nfl-draft-englands-travis-clayton-selected-by-buffalo-bills-in-seventh-round',\n",
    "    # mas urls aquí\n",
    "]\n",
    "data_deportes = []\n",
    "\n",
    "urls_politica = [\n",
    "    'https://apnews.com/article/biden-netanyahu-rafah-hamas-military-assistance-5c743e621c5594b49e0a89c985a605f3',\n",
    "    'https://apnews.com/article/america-first-trump-biden-russia-ukraine-policy-54080728c6e549c8312c4d71150480ba',\n",
    "    # mas urls aquí\n",
    "]\n",
    "data_politica = []\n",
    "\n",
    "urls_business = [\n",
    "    'https://www.economist.com/business/2024/05/09/for-gen-z-job-seekers-tiktok-is-the-new-linkedin',\n",
    "    'https://www.economist.com/business/2024/05/09/can-alibaba-get-the-magic-back',\n",
    "    # mas urls aquí\n",
    "]\n",
    "data_business = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a6ed5b-85af-4898-8c3b-9ef80880da8d",
   "metadata": {},
   "source": [
    "Extraemos la información necesaria de cada fuente, dependiendo de la página la información que necesitamos se encuentran en diferentes selectores y etiquetas html, por ello cada función es diferente. Recordar que las páginas para que este método funcione necesitan ser renderizadas por el servidor, puedes conocer más [aquí](https://solutionshub.epam.com/blog/post/what-is-server-side-rendering)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768a0c5e-ab71-4c0a-8b56-564309b953a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in urls_deportes: \n",
    "    # Realizar la solicitud HTTP y obtener el contenido HTML\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Buscar el titulo de la noticia\n",
    "    if (\"fox\" in url):\n",
    "        news_title = soup.find_all('h1', class_='story-title')\n",
    "    elif (\"mundo\" in url):\n",
    "        news_title = soup.find_all('h1', class_='title')\n",
    "    else:\n",
    "        news_title = soup.find_all('span', class_='sdc-article-header__long-title')\n",
    "        \n",
    "    print(\"Parsed:\", url, news_title)\n",
    "    data_deportes.append({'Title': news_title, 'Category': \"Sports\"})\n",
    "    \n",
    "for url in urls_politica: \n",
    "    # Realizar la solicitud HTTP y obtener el contenido HTML\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Buscar el titulo de la noticia\n",
    "    news_title = \"\"\n",
    "    if (\"bbc\" in url):\n",
    "        news_title = soup.find_all('h1', class_='sc-82e6a0ec-0 fxXQuy')\n",
    "    elif (\"apnews\" in url):\n",
    "        news_title = soup.find_all('h1', class_='Page-headline')\n",
    "    elif (\"nbcnews\" in url):\n",
    "        news_title = soup.find_all('h1', class_='article-hero-headline__htag lh-none-print black-print')\n",
    "    elif (\"foxnews\" in url):\n",
    "        news_title = soup.find_all('h1', class_='headline speakable')\n",
    "\n",
    "    print(\"Parsed:\", url, news_title)\n",
    "    data_politica.append({'Title': news_title, 'Category': \"Politics\"})\n",
    "    \n",
    "for url in urls_business: \n",
    "    # Realizar la solicitud HTTP y obtener el contenido HTML\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Buscar el titulo de la noticia\n",
    "    news_title = \"\"\n",
    "    if (\"bbc\" in url):\n",
    "        news_title = soup.find_all('h1', class_='sc-82e6a0ec-0 fxXQuy')\n",
    "    elif (\"economist\" in url):\n",
    "        news_title = soup.find_all('h1', class_='css-1p83fk8 e1r8fcie0')\n",
    "    elif (\"nbcnews\" in url):\n",
    "        news_title = soup.find_all('h1', class_='article-hero-headline__htag lh-none-print black-print')\n",
    "    elif (\"economictimes\" in url):\n",
    "        news_title = soup.find_all('h1', class_='artTitle font_faus')\n",
    "\n",
    "    print(\"Parsed:\", url, news_title)\n",
    "    data_business.append({'Title': news_title, 'Category': \"Business\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7b8312-8ebf-4409-9664-2c29e33f3889",
   "metadata": {},
   "source": [
    "Por último convertimos la información a data frames y luego la guardamos en un excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0d400c-ded7-476b-a3d1-069429ac2348",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(data_deportes)\n",
    "df2 = pd.DataFrame(data_politica)\n",
    "df3 = pd.DataFrame(data_business)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267aa82f-90c1-4fcf-87a9-da83d0430e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_excel('deportes.xlsx', index=False)\n",
    "df2.to_excel('politica.xlsx', index=False)\n",
    "df3.to_excel('negocios.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
